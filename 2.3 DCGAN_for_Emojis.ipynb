{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import imageio\n",
        "from glob import glob\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "# --- Constants and Configuration ---\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "IMG_WIDTH = 32\n",
        "IMG_HEIGHT = 32\n",
        "IMG_CHANNELS = 3\n",
        "LATENT_DIM = 100\n",
        "ITERATIONS = 20000\n",
        "FID_SAMPLE_SIZE = 3000\n",
        "\n",
        "# --- 1. Data Loading and Preparation ---\n",
        "def load_emoji_dataset(data_dir):\n",
        "    \"\"\"\n",
        "    Loads images from the specified directory and prepares the tf.data.Dataset.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Error: The directory '{data_dir}' does not exist.\")\n",
        "        print(\"Please download and unzip the EmojiOne dataset and place the '32' folder\")\n",
        "        print(\"at the correct path.\")\n",
        "        return None\n",
        "\n",
        "    # Use glob to find all png files\n",
        "    image_paths = glob(os.path.join(data_dir, '*.png'))\n",
        "    print(f\"Found {len(image_paths)} images.\")\n",
        "\n",
        "    def load_and_preprocess_image(path):\n",
        "        image = tf.io.read_file(path)\n",
        "        # Decode PNG, it might have 4 channels (RGBA)\n",
        "        image = tf.image.decode_png(image, channels=4)\n",
        "        # Drop the alpha channel\n",
        "        image = image[:, :, :3]\n",
        "        image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "        # Normalize the images to [-1, 1]\n",
        "        image = (tf.cast(image, tf.float32) - 127.5) / 127.5\n",
        "        return image\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    return dataset\n",
        "\n",
        "# --- 2. DCGAN Model Architecture ---\n",
        "\n",
        "def make_generator_model():\n",
        "    \"\"\"Builds the Generator model based on the specified architecture.\"\"\"\n",
        "    model = tf.keras.Sequential()\n",
        "    # Input: z (noise vector)\n",
        "    model.add(layers.Dense(4*4*512, use_bias=False, input_shape=(LATENT_DIM,)))\n",
        "    model.add(layers.Reshape((4, 4, 512)))\n",
        "    assert model.output_shape == (None, 4, 4, 512)\n",
        "\n",
        "    # Upsampling block 1\n",
        "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 8, 8, 256)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Upsampling block 2\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 16, 16, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Output block\n",
        "    model.add(layers.Conv2DTranspose(IMG_CHANNELS, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    \"\"\"Builds the Discriminator model based on the specified architecture.\"\"\"\n",
        "    model = tf.keras.Sequential()\n",
        "    # Input: 32x32x3 image\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Conv block 1\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Conv block 2\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Output\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1)) # No sigmoid for LSGAN\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 3. Loss Functions and Optimizers ---\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    \"\"\"Least Squares loss for the discriminator.\"\"\"\n",
        "    real_loss = tf.reduce_mean(tf.math.square(real_output - 1.0))\n",
        "    fake_loss = tf.reduce_mean(tf.math.square(fake_output))\n",
        "    total_loss = 0.5 * (real_loss + fake_loss)\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    \"\"\"Least Squares loss for the generator.\"\"\"\n",
        "    return tf.reduce_mean(tf.math.square(fake_output - 1.0))\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "\n",
        "# A single training step\n",
        "@tf.function\n",
        "def train_step(images, generator, discriminator):\n",
        "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input, output_dir):\n",
        "    \"\"\"Generates images and saves them as a grid.\"\"\"\n",
        "    predictions = model(test_input, training=False)\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        # Denormalize image from [-1, 1] to [0, 1]\n",
        "        plt.imshow((predictions[i, :, :, :] + 1.0) / 2.0)\n",
        "        plt.axis('off')\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    plt.savefig(os.path.join(output_dir, f'image_at_iteration_{epoch:05d}.png'))\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def train(dataset, iterations, generator, discriminator, output_dir):\n",
        "    \"\"\"The main training loop.\"\"\"\n",
        "    seed = tf.random.normal([16, LATENT_DIM]) # Fixed noise for visualization\n",
        "\n",
        "    data_iterator = iter(dataset.repeat())\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        start = time.time()\n",
        "\n",
        "        # Get next batch of real images\n",
        "        real_images = next(data_iterator)\n",
        "\n",
        "        # Perform a training step\n",
        "        gen_loss, disc_loss = train_step(real_images, generator, discriminator)\n",
        "\n",
        "        # Save images every 200 iterations\n",
        "        if (iteration + 1) % 200 == 0:\n",
        "            generate_and_save_images(generator, iteration + 1, seed, output_dir)\n",
        "            print(f'Iteration {iteration + 1}, Gen Loss: {gen_loss}, Disc Loss: {disc_loss}, Time: {time.time()-start:.2f} sec')\n",
        "\n",
        "    # Final generation\n",
        "    generate_and_save_images(generator, iterations, seed, output_dir)\n",
        "\n",
        "\n",
        "# --- 5. FID Calculation ---\n",
        "\n",
        "def calculate_fid(model, real_images, generated_images):\n",
        "    \"\"\"Calculates the Fr√©chet Inception Distance.\"\"\"\n",
        "    # Load InceptionV3 model pre-trained on ImageNet\n",
        "    inception_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                        pooling='avg',\n",
        "                                                        input_shape=(75,75,3)) # FID uses a minimum size of 75x75\n",
        "\n",
        "    def preprocess_for_inception(images):\n",
        "        # Resize images to 75x75 as required by InceptionV3 for FID\n",
        "        images_resized = tf.image.resize(images, (75, 75), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        # InceptionV3 preprocesses from [0, 255] range\n",
        "        images_rescaled = (images_resized + 1.0) * 127.5\n",
        "        return tf.keras.applications.inception_v3.preprocess_input(images_rescaled)\n",
        "\n",
        "    # Preprocess images\n",
        "    real_images_preprocessed = preprocess_for_inception(real_images)\n",
        "    generated_images_preprocessed = preprocess_for_inception(generated_images)\n",
        "\n",
        "    # Calculate activations\n",
        "    act1 = inception_model.predict(real_images_preprocessed)\n",
        "    act2 = inception_model.predict(generated_images_preprocessed)\n",
        "\n",
        "    # Calculate mean and covariance statistics\n",
        "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "\n",
        "    # Calculate sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\n",
        "    # Calculate sqrt of product of cov\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "    # Check and correct for imaginary numbers from sqrtm\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    # Calculate score\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == '__main__':\n",
        "    # --- IMPORTANT ---\n",
        "    # 1. Download the emoji dataset from one of the provided links.\n",
        "    # 2. Unzip the file.\n",
        "    # 3. Place the folder named '32' (from joypixels-7.0-free/png/unicode/32)\n",
        "    #    into the same directory as this script, or update the path below.\n",
        "    DATA_DIR = '32'\n",
        "    OUTPUT_DIR = 'dcgan_training_output'\n",
        "\n",
        "    # Load dataset\n",
        "    emoji_dataset = load_emoji_dataset(DATA_DIR)\n",
        "\n",
        "    if emoji_dataset:\n",
        "        # Create models\n",
        "        generator = make_generator_model()\n",
        "        discriminator = make_discriminator_model()\n",
        "\n",
        "        # Train the DCGAN\n",
        "        print(\"\\n--- Starting DCGAN Training ---\")\n",
        "        train(emoji_dataset, ITERATIONS, generator, discriminator, OUTPUT_DIR)\n",
        "        print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "        # --- FID Evaluation ---\n",
        "        print(\"\\n--- Starting FID Evaluation ---\")\n",
        "        # 1. Generate images\n",
        "        print(f\"Generating {FID_SAMPLE_SIZE} images for FID calculation...\")\n",
        "        noise = tf.random.normal([FID_SAMPLE_SIZE, LATENT_DIM])\n",
        "        generated_images_for_fid = generator(noise, training=False)\n",
        "\n",
        "        # 2. Load real images\n",
        "        print(f\"Loading {FID_SAMPLE_SIZE} real images for FID calculation...\")\n",
        "        real_images_for_fid = []\n",
        "        for batch in emoji_dataset.take( (FID_SAMPLE_SIZE // BATCH_SIZE) + 1 ):\n",
        "            for img in batch:\n",
        "                real_images_for_fid.append(img)\n",
        "        real_images_for_fid = tf.stack(real_images_for_fid)[:FID_SAMPLE_SIZE]\n",
        "\n",
        "        # 3. Calculate FID\n",
        "        fid_score = calculate_fid(None, real_images_for_fid, generated_images_for_fid)\n",
        "        print(f'\\n---> Fr√©chet Inception Distance (FID): {fid_score:.2f} <---')\n",
        "        print(\"(Lower is better)\")"
      ],
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2570 images.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting DCGAN Training ---\n",
            "Iteration 200, Gen Loss: 0.5689087510108948, Disc Loss: 0.12533392012119293, Time: 7.46 sec\n",
            "Iteration 400, Gen Loss: 0.815582275390625, Disc Loss: 0.08820968866348267, Time: 8.94 sec\n",
            "Iteration 600, Gen Loss: 0.4354490339756012, Disc Loss: 0.14376750588417053, Time: 8.64 sec\n",
            "Iteration 800, Gen Loss: 0.6004880666732788, Disc Loss: 0.0839281976222992, Time: 7.35 sec\n",
            "Iteration 1000, Gen Loss: 0.9469025135040283, Disc Loss: 0.13228042423725128, Time: 8.73 sec\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_cerbdPlz0S",
        "outputId": "a0c88e9b-4faf-4974-f681-36cb70c830ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "RW8c7v5Wmigi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}